{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSlpaN6mbmun"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1750855695885,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "2CzyGPioBUhL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBZo-t50bqZ-"
   },
   "source": [
    "# Detecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1750855698297,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "f_a_vuFOB3IV",
    "outputId": "5fc23d6d-f50b-46a0-b2b8-81b3171d84da"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def find_header_row(file_path, num_rows_to_check=50):\n",
    "\n",
    "    try:\n",
    "        temp_df = pd.read_excel(file_path, header=None, nrows=num_rows_to_check)\n",
    "\n",
    "        best_header_row_index = 0\n",
    "        max_score = -1\n",
    "        for i in range(len(temp_df)):\n",
    "            potential_header_series = temp_df.iloc[i]\n",
    "            numeric_check = pd.to_numeric(potential_header_series, errors='coerce')\n",
    "            non_numeric_count = numeric_check.isna().sum()\n",
    "            row_as_strings = potential_header_series.astype(str)\n",
    "            unique_non_empty_strings_count = row_as_strings[row_as_strings.str.strip() != ''].nunique()\n",
    "            current_score = (unique_non_empty_strings_count * 3) + non_numeric_count\n",
    "            if current_score > max_score:\n",
    "                max_score = current_score\n",
    "                best_header_row_index = i\n",
    "\n",
    "        print(f\"Detected header row (0-indexed): {best_header_row_index}\")\n",
    "        return best_header_row_index\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during header detection: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "file_path = 'auditsamplev3.xlsx'\n",
    "\n",
    "header_row_index = find_header_row(file_path)\n",
    "df = pd.read_excel(file_path, header=header_row_index)\n",
    "columns = df.columns\n",
    "\n",
    "print(\"\\nColumns in the DataFrame:\")\n",
    "print(columns)\n",
    "\n",
    "columns_list = columns.tolist()\n",
    "print(\"\\nColumns as a Python list:\")\n",
    "print(columns_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1750855699174,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "RvPCUpdZElW2",
    "outputId": "5a8fcba5-2018-44f3-be54-9fe4e22ed92c"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ext3653fRuIw"
   },
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El8_w3zjR0J7"
   },
   "source": [
    "## 1. Standardize column names: Remove leading/trailing whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750855701372,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "x3jl8CfmR65z",
    "outputId": "fce20f83-c75f-4e72-f2c1-e91fc55377d0"
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "print(\"Columns stripped of whitespace.\")\n",
    "print(f\"Current columns after stripping: {df.columns.tolist()}\")\n",
    "df_cleaned = df.copy()\n",
    "print(\"Created a copy of the DataFrame for cleaning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXNZdlTsSTw0"
   },
   "source": [
    "## 2. Handle 'N/A' values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1750855702729,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "w-A2wIPzSWUY",
    "outputId": "2172f032-cea8-40d8-9e48-04275c8bc89e"
   },
   "outputs": [],
   "source": [
    "na_values_to_replace = ['N/A', 'n/a', 'NA', 'N.A.', 'not applicable', '-']\n",
    "for col in df_cleaned.select_dtypes(include='object').columns:\n",
    "        df_cleaned[col] = df_cleaned[col].astype(str)\n",
    "        df_cleaned[col] = df_cleaned[col].replace(na_values_to_replace, np.nan)\n",
    "        df_cleaned[col] = df_cleaned[col].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "print(\"Common 'N/A' and empty string values replaced with NaN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9uESPDKNSgz8"
   },
   "source": [
    "## 3. Identify and filter 'Dead Links / Redirects'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1750855704123,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "sP_2GvgrSl9v",
    "outputId": "018cca78-9208-4052-ec70-024c2635a086"
   },
   "outputs": [],
   "source": [
    "traffic_volume_col = 'Level of Traffic Volume'\n",
    "if traffic_volume_col in df_cleaned.columns:\n",
    "        inactive_site_indicators = ['N/A - Dead Links / Redirects', 'Dead Links', 'Redirects']\n",
    "        active_sites = df_cleaned[~df_cleaned[traffic_volume_col].isin(inactive_site_indicators)].copy()\n",
    "        print(f\"Filtered out inactive sites based on '{traffic_volume_col}'.\")\n",
    "        print(f\"Original rows: {len(df_cleaned)}, Active rows: {len(active_sites)}\")\n",
    "else:\n",
    "        print(f\"Warning: Column '{traffic_volume_col}' not found. Skipping filtering for active sites.\")\n",
    "        active_sites = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCmOo-PMSthQ"
   },
   "source": [
    "## 4. Handle 'Any Score' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1750855705610,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "KC8gJAZrCN-t",
    "outputId": "3264878b-9bf9-4a46-a32d-5552c28fe5e0"
   },
   "outputs": [],
   "source": [
    "def parse_score_advanced(score_str):\n",
    "\n",
    "    if pd.isna(score_str) or str(score_str).strip() == '' or str(score_str).strip().lower() in [s.lower() for s in na_values_to_replace]:\n",
    "        return np.nan\n",
    "    s = str(score_str).strip().lower()\n",
    "\n",
    "    # Case 1: \"X out of Y\" format (e.g., \"75 out of 100\", \"15.5 out of 20\")\n",
    "    match_out_of = re.match(r'(\\d+(?:\\.\\d+)?)\\s*out of\\s*(\\d+(?:\\.\\d+)?)', s)\n",
    "    if match_out_of:\n",
    "        try:\n",
    "            numerator = float(match_out_of.group(1))\n",
    "            denominator = float(match_out_of.group(2))\n",
    "            if denominator != 0:\n",
    "                return (numerator / denominator) * 100.0\n",
    "            else:\n",
    "                return np.nan\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Case 2: Percentage format (e.g., \"85.5 %\", \"92%\")\n",
    "    match_percent = re.match(r'(\\d+(?:\\.\\d+)?)\\s*%', s)\n",
    "    if match_percent:\n",
    "        try:\n",
    "            return float(match_percent.group(1)) * 1.0\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    # Case 3: Normal numeric format (e.g., \"75\", \"120.5\")\n",
    "    try:\n",
    "        return float(s)\n",
    "    except ValueError:\n",
    "        pass # Fall through if conversion fails\n",
    "\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "score_columns = [col for col in active_sites.columns if 'score' in col.lower()]\n",
    "\n",
    "if score_columns:\n",
    "    print(f\"Found score-related columns: {score_columns}\")\n",
    "    for col_name in score_columns:\n",
    "        active_sites[col_name] = active_sites[col_name].astype(str)\n",
    "        active_sites[col_name] = active_sites[col_name].apply(parse_score_advanced)\n",
    "        print(f\"Cleaned '{col_name}' and converted to float (double).\")\n",
    "        print(f\"'{col_name}' Dtype: {active_sites[col_name].dtype}\")\n",
    "else:\n",
    "    print(\"No columns containing 'score' were found. Skipping cleaning for such columns.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Cleaned 'Score' columns preview ---\")\n",
    "print(active_sites[score_columns] if score_columns else active_sites)\n",
    "print(\"\\nData types after Score column cleaning:\")\n",
    "print(active_sites.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m6ux_mWDWUZP"
   },
   "source": [
    "## 5. Normalize Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1750855707253,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "LDORq8y1WTWH"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def normalize_col_name(col_name):\n",
    "    normalized = col_name.lower().strip()\n",
    "    normalized = normalized.replace(' / ', '/').replace(' ', '_')\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    sanitized = re.sub(r'[^\\w\\s-]', '_', filename.strip())\n",
    "    sanitized = re.sub(r'\\s+', '_', sanitized)\n",
    "    return sanitized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKh7bJp3WuRM"
   },
   "source": [
    "## 6. Remove Duplicate Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1750855708731,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "Dl6E6-NUZUmZ",
    "outputId": "4bec6a34-d943-40bb-bb73-941dd8f40191"
   },
   "outputs": [],
   "source": [
    "print(\"Original DataFrame shape:\", active_sites.shape)\n",
    "# active_sites = active_sites.drop_duplicates(subset=['Website Name / Domain Name'], keep='first')\n",
    "print(\"\\nDataFrame shape after removing duplicates:\", active_sites.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN42JCQ2S5pj"
   },
   "source": [
    "## 7. General categorical column cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44,
     "status": "ok",
     "timestamp": 1750855710174,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "51-uoklxDnVn",
    "outputId": "c677059a-ff99-4142-e000-ede444603f33"
   },
   "outputs": [],
   "source": [
    "categorical_cols = active_sites.select_dtypes(include=['object']).columns\n",
    "\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"\\nFound potential categorical columns for cleaning: {list(categorical_cols)}\")\n",
    "    for col_name in categorical_cols:\n",
    "        active_sites[col_name] = active_sites[col_name].astype(str).str.strip()\n",
    "        active_sites[col_name] = active_sites[col_name].replace(r'^\\s*$', np.nan, regex=True)\n",
    "        print(f\"Cleaned '{col_name}' by stripping whitespace and replacing empty strings with NaN.\")\n",
    "else:\n",
    "    print(\"\\nNo columns with 'object' dtype found to clean as categorical.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Cleaned DataFrame Info ---\")\n",
    "active_sites.info()\n",
    "print(\"\\n--- Cleaned DataFrame Head ---\")\n",
    "print(active_sites.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtHM880dTDYM"
   },
   "source": [
    "## 8. General numerical column cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1750855711974,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "iV9sNjuITIJh",
    "outputId": "b73082d9-9da8-4bc9-9402-8952e9ec418d"
   },
   "outputs": [],
   "source": [
    "for col in active_sites.columns:\n",
    "        converted_col = pd.to_numeric(active_sites[col], errors='coerce')\n",
    "        if not converted_col.isnull().all() and converted_col.dtype != active_sites[col].dtype:\n",
    "            active_sites[col] = converted_col\n",
    "            print(f\"Attempted to convert column '{col}' to numeric. New Dtype: {active_sites[col].dtype}\")\n",
    "\n",
    "print(\"\\n--- Cleaning Summary ---\")\n",
    "print(\"First 5 rows of the cleaned 'active_sites' DataFrame:\")\n",
    "print(active_sites.head())\n",
    "print(\"\\nData types of 'active_sites' DataFrame:\")\n",
    "print(active_sites.info())\n",
    "print(f\"\\nShape of the cleaned DataFrame (active_sites): {active_sites.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "executionInfo": {
     "elapsed": 110,
     "status": "ok",
     "timestamp": 1750855713020,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "Dfxx9USsRWUP",
    "outputId": "948abf4d-d16a-4a35-885e-8d83aff61e01"
   },
   "outputs": [],
   "source": [
    "active_sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1750855713826,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "Yhwg97WTaPaN",
    "outputId": "0286191d-1b8a-47fd-9ed7-6242f693ca2e"
   },
   "outputs": [],
   "source": [
    "active_sites.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sazyv-oWf1h"
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Atb2MRnaKF"
   },
   "source": [
    "# Chart Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOH_Qfplw9Ol"
   },
   "source": [
    "# PPT Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11504,
     "status": "ok",
     "timestamp": 1750848737994,
     "user": {
      "displayName": "Sangsriti Sarkar",
      "userId": "17170911631457279590"
     },
     "user_tz": -330
    },
    "id": "x6aX8zprvcoq",
    "outputId": "a5fd22d8-2bd3-4fba-ff02-43f4a365ad96"
   },
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install python-pptx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uXKg1WfrAx6n"
   },
   "source": [
    "### Generating PPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM-hDzYeIqBL"
   },
   "source": [
    "## Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import io\n",
    "import re\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from pptx import Presentation\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.text import PP_ALIGN\n",
    "from pptx.util import Emu, Pt\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "def normalize_col_name(col_name: str) -> str:\n",
    "    \"\"\"Normalize column name for comparison.\"\"\"\n",
    "    return col_name.lower().replace('_', '').replace('-', '').replace(' ', '')\n",
    "\n",
    "def get_risk_counts(scores: pd.Series) -> Tuple[int, int, int]:\n",
    "    \"\"\"Return count of high, medium, low risk.\"\"\"\n",
    "    high = (scores < 60).sum()\n",
    "    medium = ((scores >= 60) & (scores < 90)).sum()\n",
    "    low = (scores >= 90).sum()\n",
    "    return high, medium, low\n",
    "\n",
    "def describe_distribution(series: pd.Series) -> str:\n",
    "    skew = series.skew()\n",
    "    if skew > 0.5:\n",
    "        return \"right-skewed (most values below average)\"\n",
    "    elif skew < -0.5:\n",
    "        return \"left-skewed (most values above average)\"\n",
    "    else:\n",
    "        return \"normally distributed\"\n",
    "\n",
    "def generate_bullet_points_for_chart(\n",
    "    df: pd.DataFrame, df_original: pd.DataFrame, col: str, chart_type: str\n",
    ") -> List[str]:\n",
    "    \"\"\"Generate contextual bullet points based on data analysis.\"\"\"\n",
    "    bullet_points = []\n",
    "    valid_data = df[col].dropna()\n",
    "    if chart_type == \"risk_distribution\" and not valid_data.empty:\n",
    "        high, medium, low = get_risk_counts(valid_data)\n",
    "        total = len(valid_data)\n",
    "        high_pct, low_pct = (high/total)*100, (low/total)*100\n",
    "        mean_score = valid_data.mean()\n",
    "        bullet_points = [\n",
    "            f\"Total {total} records analyzed with average score of {mean_score:.1f}\",\n",
    "            f\"High risk sites represent {high_pct:.1f}% ({high}) of total records\",\n",
    "            f\"Low risk sites account for {low_pct:.1f}% ({low}) of total records\",\n",
    "            f\"Immediate attention needed for {high} high-risk sites\"\n",
    "        ]\n",
    "    elif chart_type == \"score_distribution\" and not valid_data.empty:\n",
    "        mean, median, std = valid_data.mean(), valid_data.median(), valid_data.std()\n",
    "        dist_desc = describe_distribution(valid_data)\n",
    "        min_val, max_val = valid_data.min(), valid_data.max()\n",
    "        bullet_points = [\n",
    "            f\"Scores range from {min_val:.1f} to {max_val:.1f} with a mean of {mean:.1f}.\",\n",
    "            f\"Standard deviation of {std:.1f} shows {'high' if std > mean * 0.3 else 'moderate'} variability.\",\n",
    "            f\"The distribution is {dist_desc}.\",\n",
    "            f\"Median of {median:.1f} is {'close to' if abs(mean-median)<std*0.1 else 'distinct from'} the mean.\"\n",
    "        ]\n",
    "    elif chart_type == \"numerical_distribution\" and not valid_data.empty:\n",
    "        mean, median = valid_data.mean(), valid_data.median()\n",
    "        q25, q75 = valid_data.quantile(0.25), valid_data.quantile(0.75)\n",
    "        bullet_points = [\n",
    "            f\"The dataset has {len(valid_data)} valid entries and {len(df_original)-len(valid_data)} dead links (about {((len(df_original)-len(valid_data))/len(df_original))*100:.1f}%)\",\n",
    "            f\"The average value is {mean:.2f}, with a median of {median:.2f}.\",\n",
    "            f\"The 25th percentile is {q25:.2f}, and the 75th percentile is {q75:.2f}.\",\n",
    "            f\"The middle 50% of data lies between {q25:.2f} and {q75:.2f}.\"\n",
    "        ]\n",
    "    elif chart_type == \"categorical_pie\":\n",
    "        value_counts = df[col].value_counts()\n",
    "        total_count = len(valid_data)\n",
    "        top_category, top_count = value_counts.index[0], value_counts.iloc[0]\n",
    "        top_pct = (top_count/total_count)*100\n",
    "        bullet_points = [\n",
    "            f\"The dataset contains {len(value_counts)} distinct categories across {total_count} records.\",\n",
    "            f\"The dominant category '{top_category}' has {top_count} occurrences ({top_pct:.1f}%).\",\n",
    "            f\"The distribution is {'relatively uniform' if top_pct < 40 else 'skewed towards a few dominant categories'}.\",     \n",
    "            f\"Category representation is {'fairly consistent' if value_counts.std() < value_counts.mean() * 0.5 else 'highly variable'}.\"\n",
    "        ]\n",
    "    elif chart_type == \"categorical_bar\":\n",
    "        value_counts = df[col].value_counts()\n",
    "        total_count = len(valid_data)\n",
    "        top_10_count = value_counts.head(10).sum()\n",
    "        top_category, top_pct = value_counts.index[0], (value_counts.iloc[0]/total_count)*100\n",
    "        bullet_points = [\n",
    "            f\"The column {col} has {len(value_counts)} unique categories across {total_count} records.\",\n",
    "            f\"The top category '{top_category}' accounts for {top_pct:.1f}% of the data.\",\n",
    "            f\"The top 10 categories make up {(top_10_count/total_count)*100:.1f}% of all entries.\",\n",
    "            f\"The categories are {'spread out across many values' if len(value_counts) > total_count * 0.5 else 'mostly focused on a few values'}.\"\n",
    "        ]\n",
    "    return bullet_points[:4]\n",
    "\n",
    "def generate_hexbin_bullet_points(df: pd.DataFrame, x_col: str, y_col: str) -> List[str]:\n",
    "    corr = df[x_col].corr(df[y_col])\n",
    "    return [\n",
    "        \"Darker hexagons indicate denser data regions, while lighter ones show sparser observations.\",\n",
    "        f\"A {'positive' if corr > 0 else 'negative' if corr < 0 else 'no'} linear trend exists (Pearson correlation coefficient: {corr:.2f}).\",\n",
    "        f\"{x_col} ranges from {df[x_col].min():.1f} to {df[x_col].max():.1f}, and {y_col} ranges from {df[y_col].min():.1f} to {df[y_col].max():.1f}.\"\n",
    "    ]\n",
    "\n",
    "def add_conclusion(df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"Generate summary insights based on risk and volume columns.\"\"\"\n",
    "    volume_cols = [col for col in df.columns if \"volume\" in col.lower()]\n",
    "    str_volume_cols = [col for col in volume_cols if df[col].apply(type).eq(str).any()]\n",
    "    score_cols = [col for col in df.columns if \"score\" in col.lower()]\n",
    "    name_cols = [col for col in df.columns if any(keyword in col.lower() for keyword in [\"website\", \"domain\", \"site\", \"link\"])]\n",
    "\n",
    "    matching_rows = pd.DataFrame()\n",
    "    for vol_col in str_volume_cols:\n",
    "        for score_col in score_cols:\n",
    "            mask = df[vol_col].str.contains(\"high\", case=False, na=False) & (df[score_col] < 50)\n",
    "            filtered = df.loc[mask, name_cols]\n",
    "            matching_rows = pd.concat([matching_rows, filtered], ignore_index=True)\n",
    "    result = matching_rows.drop_duplicates().head(10)\n",
    "    if not result.empty:\n",
    "        first_name_col = result.columns[0]\n",
    "        top_sites = result[first_name_col].dropna().unique()[:9]\n",
    "        if len(top_sites) > 0:\n",
    "            site_list = ', '.join(top_sites[:-1]) + f', and {top_sites[-1]}' if len(top_sites) > 1 else top_sites[0]\n",
    "            return [\n",
    "                f\"The current dataset includes the following key columns: {', '.join(df.columns)}\",\n",
    "                f\"The sites {site_list} are considered high-risk given their low {score_cols[0].lower()} despite having high {str_volume_cols[0].lower()}.\"\n",
    "            ]\n",
    "    return [f\"The current dataset includes the following key columns: {', '.join(df.columns)}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization Functions ---\n",
    "\n",
    "def plot_numerical_histogram(ax, series: pd.Series, col: str, clean_title: str):\n",
    "    mean_val, median_val, std_val = series.mean(), series.median(), series.std()\n",
    "    sns.histplot(series, kde=True, color='cornflowerblue', bins=30, ax=ax)\n",
    "    ax.axvline(mean_val, color='blue', linestyle='--', label=f'Mean: {mean_val:.2f}')\n",
    "    ax.axvline(median_val, color='red', linestyle='--', label=f'Median: {median_val:.2f}')\n",
    "    ax.axvline(mean_val + std_val, color='purple', linestyle=':', label=f'+-1 Std Dev: {std_val:.2f}')\n",
    "    ax.axvline(mean_val - std_val, color='purple', linestyle=':')\n",
    "    ax.set_title(f'Distribution of {clean_title}', fontsize=16, pad=20)\n",
    "    ax.set_xlabel(col, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.legend()\n",
    "\n",
    "def plot_categorical_bar(ax, value_counts: pd.Series, col: str, clean_title: str):\n",
    "    max_label_length = 30\n",
    "    sorted_index = value_counts.reset_index()\n",
    "    sorted_index.columns = ['label', 'count']\n",
    "    sorted_index[\"label_length\"] = sorted_index[\"label\"].astype(str).apply(len)\n",
    "    sorted_index = sorted_index.sort_values(by=[\"count\", \"label_length\", \"label\"], ascending=[False, True, True])\n",
    "    top_10_df = sorted_index.head(10).copy()\n",
    "    top_10_df[\"short_label\"] = top_10_df[\"label\"].astype(str).apply(\n",
    "        lambda x: x if len(x) <= max_label_length else x[:max_label_length] + \"…\"\n",
    "    )\n",
    "    sns.barplot(\n",
    "        x=top_10_df[\"count\"],\n",
    "        y=top_10_df[\"short_label\"],\n",
    "        hue=top_10_df[\"short_label\"],\n",
    "        palette=\"Set2\",\n",
    "        orient=\"h\",\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    ax.set_title(f'Top 10 Most Frequent Values in {clean_title}', fontsize=16, pad=20)\n",
    "    ax.set_xlabel('Count', fontsize=12)\n",
    "    ax.set_ylabel(col, fontsize=12)\n",
    "\n",
    "def visualize_column_summary(\n",
    "    active_sites_df: pd.DataFrame, df_original: pd.DataFrame\n",
    ") -> Tuple[List[Tuple[Any, str, List[str]]], List[str]]:\n",
    "    if not isinstance(active_sites_df, pd.DataFrame):\n",
    "        print(\"Error: Input is not a valid pandas DataFrame.\")\n",
    "        return [], []\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    cols_to_exclude = [normalize_col_name('id')]\n",
    "    score_keyword = 'score'\n",
    "    cols_for_viz = [col for col in active_sites_df.columns if normalize_col_name(col) not in cols_to_exclude]\n",
    "    chart_data = []\n",
    "    num_rows = len(active_sites_df)\n",
    "\n",
    "    for col in cols_for_viz:\n",
    "        clean_title = col.replace('_', ' ').replace('-', ' ').title()\n",
    "        if pd.api.types.is_numeric_dtype(active_sites_df[col]):\n",
    "            if score_keyword in normalize_col_name(col):\n",
    "                valid_scores = active_sites_df[col].dropna()\n",
    "                if not valid_scores.empty:\n",
    "                    # Pie chart: risk distribution\n",
    "                    fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
    "                    high, medium, low = get_risk_counts(valid_scores)\n",
    "                    risk_counts = [high, medium, low]\n",
    "                    risk_labels = [\n",
    "                        f'High Risk (< 60)', f'Medium Risk (60-89)', f'Low Risk (>= 90)'\n",
    "                    ]\n",
    "                    colors = ['#FF6347', '#FFD700', '#90EE90']\n",
    "                    explode_values = [0.05 if i == 0 and high > 0 else 0 for i in range(3)]\n",
    "                    ax1.pie(\n",
    "                        risk_counts, labels=risk_labels, autopct='%1.1f%%', startangle=140,\n",
    "                        colors=colors, wedgeprops={'edgecolor': 'white'}, shadow=True, explode=explode_values\n",
    "                    )\n",
    "                    ax1.set_title(f'{clean_title} Risk Distribution', fontsize=16, pad=20)\n",
    "                    ax1.set_ylabel('')\n",
    "                    fig1.tight_layout()\n",
    "                    chart_data.append((\n",
    "                        fig1, f\"{clean_title} Risk Distribution\",\n",
    "                        generate_bullet_points_for_chart(active_sites_df, df_original, col, \"risk_distribution\")\n",
    "                    ))\n",
    "                    plt.close(fig1)\n",
    "\n",
    "                    # Histogram: score distribution\n",
    "                    fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
    "                    plot_numerical_histogram(ax2, valid_scores, col, clean_title)\n",
    "                    fig2.tight_layout()\n",
    "                    chart_data.append((\n",
    "                        fig2, f\"{clean_title} Distribution\",\n",
    "                        generate_bullet_points_for_chart(active_sites_df, df_original, col, \"score_distribution\")\n",
    "                    ))\n",
    "                    plt.close(fig2)\n",
    "            else:\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                plot_numerical_histogram(ax, active_sites_df[col].dropna(), col, clean_title)\n",
    "                fig.tight_layout()\n",
    "                chart_data.append((\n",
    "                    fig, f\"{clean_title} Distribution\",\n",
    "                    generate_bullet_points_for_chart(active_sites_df, df_original, col, \"numerical_distribution\")\n",
    "                ))\n",
    "                plt.close(fig)\n",
    "        else:\n",
    "            unique_count = active_sites_df[col].nunique()\n",
    "            if unique_count == num_rows or unique_count == 0:\n",
    "                continue  # Skip unique or empty\n",
    "            if unique_count <= 5:\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                value_counts = active_sites_df[col].value_counts()\n",
    "                explode_values = [0.05] * len(value_counts)\n",
    "                ax.pie(\n",
    "                    value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=140,\n",
    "                    wedgeprops={'edgecolor': 'white'}, shadow=True, explode=explode_values\n",
    "                )\n",
    "                ax.set_title(f'Distribution of {clean_title}', fontsize=16, pad=20)\n",
    "                ax.set_ylabel('')\n",
    "                fig.tight_layout()\n",
    "                chart_data.append((\n",
    "                    fig, f\"{clean_title} Distribution\",\n",
    "                    generate_bullet_points_for_chart(active_sites_df, df_original, col, \"categorical_pie\")\n",
    "                ))\n",
    "                plt.close(fig)\n",
    "            else:\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                value_counts = active_sites_df[col].value_counts()\n",
    "                plot_categorical_bar(ax, value_counts, col, clean_title)\n",
    "                fig.tight_layout()\n",
    "                chart_data.append((\n",
    "                    fig, f\"Top 10 {clean_title} Values\",\n",
    "                    generate_bullet_points_for_chart(active_sites_df, df_original, col, \"categorical_bar\")\n",
    "                ))\n",
    "                plt.close(fig)\n",
    "\n",
    "    # Hexbin plot (traffic vs score)\n",
    "    traffic_col, score_col = None, None\n",
    "    for col in active_sites_df.columns:\n",
    "        norm_col = normalize_col_name(col)\n",
    "        if pd.api.types.is_numeric_dtype(active_sites_df[col]):\n",
    "            if 'traffic' in norm_col:\n",
    "                traffic_col = col\n",
    "            elif 'score' in norm_col:\n",
    "                score_col = col\n",
    "    if traffic_col and score_col:\n",
    "        x, y = active_sites_df[traffic_col].dropna(), active_sites_df[score_col].dropna()\n",
    "        df_hex = pd.DataFrame({traffic_col: x, score_col: y}).dropna()\n",
    "        if not df_hex.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            hb = ax.hexbin(df_hex[traffic_col], df_hex[score_col], gridsize=30, cmap='viridis_r', mincnt=1)\n",
    "            cb = fig.colorbar(hb, ax=ax)\n",
    "            cb.set_label('Count')\n",
    "            ax.set_xlabel(traffic_col)\n",
    "            ax.set_ylabel(score_col)\n",
    "            ax.set_title(f'{traffic_col} vs {score_col} Hexbin Plot', fontsize=16, pad=20)\n",
    "            fig.tight_layout()\n",
    "            chart_title = f\"{traffic_col.replace('_',' ').title()} vs {score_col.replace('_',' ').title()} Relationship\"\n",
    "            chart_data.append((fig, chart_title, generate_hexbin_bullet_points(df_hex, traffic_col, score_col)))\n",
    "            plt.close(fig)\n",
    "\n",
    "    return chart_data, add_conclusion(active_sites_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PPTX Creation Functions ---\n",
    "\n",
    "# Layout Constants\n",
    "TITLE_FONT_NAME = 'Times New Roman'\n",
    "TITLE_FONT_COLOR = RGBColor(112, 48, 160)\n",
    "TITLE_FONT_SIZE = Pt(36)\n",
    "BULLET_FONT_NAME = 'Calibri'\n",
    "BULLET_FONT_SIZE = Pt(20)\n",
    "BULLET_SPACE_AFTER = Pt(12)\n",
    "TOP_MARGIN_RATIO = 0.06\n",
    "BOTTOM_MARGIN_RATIO = 0.08\n",
    "LEFT_MARGIN_RATIO = 0.06\n",
    "RIGHT_MARGIN_RATIO = 0.06\n",
    "TITLE_HEIGHT_RATIO = 0.20\n",
    "CONTENT_HEIGHT_RATIO = 1 - TITLE_HEIGHT_RATIO\n",
    "TITLE_WIDTH_RATIO = 0.70\n",
    "TEXT_WIDTH_RATIO = 0.30\n",
    "IMAGE_WIDTH_RATIO = 0.70\n",
    "GAP_BETWEEN_TEXT_AND_IMAGE_RATIO = 0.02\n",
    "\n",
    "def bolden_values_paragraph(p, text):\n",
    "    \"\"\"Add bullet point with bolded numbers/percentages in the paragraph p.\"\"\"\n",
    "    pattern = re.compile(r\"(\\d[\\d,\\.]*%?|\\([\\d,\\.]+\\)|\\b[\\w\\-]+(?:\\.[\\w\\-]+)+\\b)\")\n",
    "    last = 0\n",
    "    for match in pattern.finditer(text):\n",
    "        if match.start() > last:\n",
    "            run = p.add_run()\n",
    "            run.text = text[last:match.start()]\n",
    "            run.font.bold = False\n",
    "            run.font.size = BULLET_FONT_SIZE\n",
    "            run.font.name = BULLET_FONT_NAME\n",
    "        run = p.add_run()\n",
    "        run.text = match.group(0)\n",
    "        run.font.bold = True\n",
    "        run.font.size = BULLET_FONT_SIZE\n",
    "        run.font.name = BULLET_FONT_NAME\n",
    "        last = match.end()\n",
    "    if last < len(text):\n",
    "        run = p.add_run()\n",
    "        run.text = text[last:]\n",
    "        run.font.bold = False\n",
    "        run.font.size = BULLET_FONT_SIZE\n",
    "        run.font.name = BULLET_FONT_NAME\n",
    "\n",
    "def add_custom_chart_slide(\n",
    "    prs: Presentation,\n",
    "    chart_fig,\n",
    "    chart_title: str,\n",
    "    bullet_points: List[str]\n",
    "):\n",
    "    slide_width, slide_height = prs.slide_width, prs.slide_height\n",
    "    usable_width = slide_width * (1 - LEFT_MARGIN_RATIO - RIGHT_MARGIN_RATIO)\n",
    "    usable_height = slide_height * (1 - TOP_MARGIN_RATIO - BOTTOM_MARGIN_RATIO)\n",
    "    usable_left = slide_width * LEFT_MARGIN_RATIO\n",
    "    usable_top = slide_height * TOP_MARGIN_RATIO\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[6])  # Blank layout\n",
    "\n",
    "    # Title\n",
    "    title_width = slide_width * TITLE_WIDTH_RATIO\n",
    "    title_left = (slide_width - title_width) / 2\n",
    "    title_top = usable_top\n",
    "    title_height = usable_height * TITLE_HEIGHT_RATIO\n",
    "    title_box = slide.shapes.add_textbox(\n",
    "        Emu(title_left), Emu(title_top), Emu(title_width), Emu(title_height)\n",
    "    )\n",
    "    tf = title_box.text_frame\n",
    "    tf.text = chart_title\n",
    "    tf.word_wrap = True\n",
    "    para = tf.paragraphs[0]\n",
    "    para.alignment = PP_ALIGN.CENTER\n",
    "    run = para.runs[0]\n",
    "    run.font.size = TITLE_FONT_SIZE\n",
    "    run.font.bold = True\n",
    "    run.font.name = TITLE_FONT_NAME\n",
    "    run.font.color.rgb = TITLE_FONT_COLOR\n",
    "\n",
    "    # Bullets\n",
    "    content_top = usable_top + usable_height * TITLE_HEIGHT_RATIO\n",
    "    content_height = usable_height * CONTENT_HEIGHT_RATIO\n",
    "    gap_width = usable_width * GAP_BETWEEN_TEXT_AND_IMAGE_RATIO\n",
    "    text_left = Emu(usable_left)\n",
    "    text_top = Emu(content_top)\n",
    "    text_width = Emu(usable_width * TEXT_WIDTH_RATIO - gap_width / 2)\n",
    "    text_height = Emu(content_height)\n",
    "    bullet_box = slide.shapes.add_textbox(text_left, text_top, text_width, text_height)\n",
    "    tf_bullets = bullet_box.text_frame\n",
    "    tf_bullets.word_wrap = True\n",
    "    for idx, txt in enumerate(bullet_points):\n",
    "        p = tf_bullets.paragraphs[0] if idx == 0 else tf_bullets.add_paragraph()\n",
    "        p.level = 0\n",
    "        p.space_after = BULLET_SPACE_AFTER\n",
    "        p.alignment = PP_ALIGN.LEFT\n",
    "        bolden_values_paragraph(p, u\"\\u2022 \" + txt)\n",
    "\n",
    "    # Image\n",
    "    image_left = Emu(usable_left + usable_width * TEXT_WIDTH_RATIO + gap_width / 2)\n",
    "    image_top = Emu(content_top)\n",
    "    image_width = Emu(usable_width * IMAGE_WIDTH_RATIO - gap_width / 2)\n",
    "    image_height = Emu(content_height)\n",
    "    img_buffer = io.BytesIO()\n",
    "    chart_fig.savefig(img_buffer, format='png', dpi=200, bbox_inches='tight')\n",
    "    img_buffer.seek(0)\n",
    "    plt.close(chart_fig)\n",
    "    img = Image.open(img_buffer)\n",
    "    img_width, img_height = img.size\n",
    "    max_width_px, max_height_px = int(image_width / 9525), int(image_height / 9525)\n",
    "    aspect = img_height / img_width\n",
    "    if max_height_px / aspect <= max_width_px:\n",
    "        final_height_px = max_height_px\n",
    "        final_width_px = int(final_height_px / aspect)\n",
    "    else:\n",
    "        final_width_px = max_width_px\n",
    "        final_height_px = int(final_width_px * aspect)\n",
    "    img_top_offset = int((max_height_px - final_height_px) / 2)\n",
    "    img_left_offset = int((max_width_px - final_width_px) / 2)\n",
    "    slide.shapes.add_picture(\n",
    "        img_buffer,\n",
    "        image_left + Emu(img_left_offset * 9525),\n",
    "        image_top + Emu(img_top_offset * 9525),\n",
    "        Emu(final_width_px * 9525),\n",
    "        Emu(final_height_px * 9525)\n",
    "    )\n",
    "\n",
    "async def build_presentation_with_charts(\n",
    "    template_path: str,\n",
    "    chart_figures_and_titles: List[Tuple[Any, str, List[str]]],\n",
    "    output_path: str,\n",
    "    insight_points: Optional[List[str]] = None\n",
    ") -> None:\n",
    "    prs = Presentation(template_path)\n",
    "    # Removing all but the first slide \n",
    "    for idx in range(len(prs.slides) - 1, 0, -1):\n",
    "        rId = prs.slides._sldIdLst[idx].rId\n",
    "        prs.slides._sldIdLst.remove(prs.slides._sldIdLst[idx])\n",
    "        prs.part.drop_rel(rId)\n",
    "    # Adding chart slides\n",
    "    for fig_obj, chart_title, chart_bullets in chart_figures_and_titles:\n",
    "        add_custom_chart_slide(prs, fig_obj, chart_title, chart_bullets)\n",
    "    # Insights slide\n",
    "    slide_width, slide_height = prs.slide_width, prs.slide_height\n",
    "    usable_width = slide_width * (1 - LEFT_MARGIN_RATIO - RIGHT_MARGIN_RATIO)\n",
    "    usable_height = slide_height * (1 - TOP_MARGIN_RATIO - BOTTOM_MARGIN_RATIO)\n",
    "    usable_left = slide_width * LEFT_MARGIN_RATIO\n",
    "    usable_top = slide_height * TOP_MARGIN_RATIO\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "    title_width = slide_width * TITLE_WIDTH_RATIO\n",
    "    title_left = (slide_width - title_width) / 2\n",
    "    title_top = usable_top\n",
    "    title_height = usable_height * TITLE_HEIGHT_RATIO\n",
    "    title_box = slide.shapes.add_textbox(\n",
    "        Emu(title_left), Emu(title_top), Emu(title_width), Emu(title_height)\n",
    "    )\n",
    "    tf = title_box.text_frame\n",
    "    tf.text = \"SUMMARY INSIGHTS\"\n",
    "    para = tf.paragraphs[0]\n",
    "    para.alignment = PP_ALIGN.CENTER\n",
    "    run = para.runs[0]\n",
    "    run.font.size = TITLE_FONT_SIZE\n",
    "    run.font.bold = True\n",
    "    run.font.name = TITLE_FONT_NAME\n",
    "    run.font.color.rgb = TITLE_FONT_COLOR\n",
    "    content_top = usable_top + usable_height * TITLE_HEIGHT_RATIO\n",
    "    content_height = usable_height * CONTENT_HEIGHT_RATIO\n",
    "    insight_left = Emu(usable_left + usable_width * 0.15)\n",
    "    insight_top = Emu(content_top)\n",
    "    insight_width = Emu(usable_width * 0.75)\n",
    "    insight_height = Emu(content_height)\n",
    "    insight_box = slide.shapes.add_textbox(insight_left, insight_top, insight_width, insight_height)\n",
    "    tf_bullets = insight_box.text_frame\n",
    "    tf_bullets.word_wrap = True\n",
    "    for idx, txt in enumerate(insight_points or []):\n",
    "        p = tf_bullets.paragraphs[0] if idx == 0 else tf_bullets.add_paragraph()\n",
    "        p.level = 0\n",
    "        p.space_after = BULLET_SPACE_AFTER\n",
    "        p.alignment = PP_ALIGN.LEFT\n",
    "        bolden_values_paragraph(p, u\"\\u2022 \" + txt)\n",
    "    # Thank you slide\n",
    "    slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "    thank_left = Emu(usable_left + usable_width * 0.15)\n",
    "    thank_top = Emu(usable_top + usable_height * 0.35)\n",
    "    thank_width = Emu(usable_width * 0.7)\n",
    "    thank_height = Emu(usable_height * 0.3)\n",
    "    thank_box = slide.shapes.add_textbox(thank_left, thank_top, thank_width, thank_height)\n",
    "    tf = thank_box.text_frame\n",
    "    tf.text = \"THANK YOU\"\n",
    "    p = tf.paragraphs[0]\n",
    "    p.alignment = PP_ALIGN.CENTER\n",
    "    run = p.runs[0]\n",
    "    run.font.size = Pt(54)\n",
    "    run.font.bold = True\n",
    "    run.font.name = TITLE_FONT_NAME\n",
    "    run.font.color.rgb = RGBColor(128, 0, 128)\n",
    "    tf.word_wrap = True\n",
    "    prs.save(output_path)\n",
    "    print(f\"Presentation created at '{output_path}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "async def main_execution(\n",
    "    active_sites: pd.DataFrame, df: pd.DataFrame,\n",
    "    template_ppt_path: str = 'GT_TA.pptx',\n",
    "    output_ppt_path: str = 'new_ppt.pptx'\n",
    "):\n",
    "    print(\"\\n--- Starting chart generation ---\\n\")\n",
    "    chart_figures_and_titles, conclusion_points = visualize_column_summary(active_sites, df)\n",
    "    print(\"\\n--- Finished chart generation ---\\n\")\n",
    "    await build_presentation_with_charts(\n",
    "        template_ppt_path,\n",
    "        chart_figures_and_titles,\n",
    "        output_ppt_path,\n",
    "        conclusion_points\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            asyncio.ensure_future(main_execution(active_sites, df))\n",
    "            print(\"PowerPoint integration task scheduled on existing event loop.\")\n",
    "        else:\n",
    "            loop.run_until_complete(main_execution(active_sites, df))\n",
    "    except RuntimeError:\n",
    "        asyncio.run(main_execution(active_sites, df))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP7abVYITU9M+kR/iyahSrY",
   "mount_file_id": "1K_PkCLgrUDv7ZtOvWM5UUTQebVjl-wvU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
